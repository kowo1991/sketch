%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[runningheads,a4paper]{llncs}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{clrscode}
\usepackage{url}
\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}
\makeatletter
    \newcommand\fcaption{\def\@captype{figure}\caption}
\makeatother
\begin{document}

\mainmatter
\title{Fusing Human Faces}
\author{Xianxiang Wang, and Xuejin Chen}
\institute{University of Science and Technology of China, 230031 Hefei, China\\
\url{kowo@mail.ustc.edu.cn}, \url{xjchen99@ustc.edu.cn}}
\toctitle{Lecture Notes in Computer Science}
\tocauthor{Authors' Instructions}
\maketitle


\begin{abstract}
Face fusion technology grows more important nowadays. In this paper, We will introduce a method that could fuse features of source face into target face, and target face would still looks natural with its background. Our method contains two steps. First, aligning two faces according to facial landmarks, for which could give us size, location and distribution  of features of two faces. Segmenting two faces into several triangles using Delaunay triangulation, warping the corresponding triangles of two faces into the same shape. Second, reconstructing gradients for new face that we are going to generate, with one face image as target image and reconstructed gradients, we could generate fused face by seamless cloning. There are three ways we tried to reconstruct the gradients, first, composing new gradients by linearly combining gradients of two faces, second,  preserving the larger gradients of two faces, third, constructing new gradients by preserving weaker gradients which is proposed by us.
\keywords{Face fusion, Facial landmarks, Delaunay triangulation, Seamless clone}
\end{abstract}


\section{Introduction}
These years, there is a great need for face editing application. Someone would wonder what would they look like if they have some face features from pop stars, they may make their faces sharper through face editing application. While some other people want to know what would they look like when they grow older. these tasks could be accomplished by face fusion. Progeny appearance prediction is a popular task lately, what if the son of a young couple looks like when he grows up, it is attracting for many people, and this could also be accomplished by face fusion technology.

As we know, different faces have different features, someone has a shaper face while the shape of other faces are closer to square. There are many ways to build correspondence of two faces. In early years, hand marked features of faces are used to build correspondence of two faces \cite{fbim}, because of the lack of the technology to detect face features automatically, when we align two faces using this method, there are a lot of labour works to do. Most people do not want to mark the features manually. Lately, a five
feature points based face fusion method \cite{mhf} was proposed, the five feature points are detected automatically, this could really save a lot of time for us, then with these feature points, two faces could be warped and fused into one. In fact, five feature points are not enough to represent a face, when we use the sparse correspondence to generate a new face, the result is weird.

Given two faces, the face we want to extract its features and fuse its features into the other face named source face, the face which keeps its background and let source face features fuse into self named target face. There are 68 facial landmarks are detected by using an Ensemble of Regression Trees \cite{fld} in our method, thus we know exactly the features and size of the face. Facial landmarks indicate the face structures, in other words, the distribution of face features, while the face gradients indicate the face details. When we fuse two faces, first, we warp two faces to make sure the structures of two faces are the same. Second, we fuse the gradients of two faces, in order to maintain the main features of the source face and illumination information of the target face, we reconstruct the gradients for new face by keeping the larger gradients of the source face and part of lower gradients of the target face, then, we could recover the fused face by seamless cloning \cite{pie}.

\section{Related work}
In this section, we summarise existing approaches that is similar to our work.

\textbf{Face Morphing.} Face morphing is commonly referred as the animated transformation of one digital face image to the other, in most case, the background of the face image is pure in order to make us focus on transition of the face itself.

 Beier and Neely \cite{fbim} developed user interface to build the correspondences of two faces by a group of line pairs, the procedure of build correspondences of two faces is tedious and time-consuming, besides, hand-marked features are not precise, after building correspondences, the corresponding pixels are linearly interpolated.  Wolberg \cite{wol} proposed a mesh-based method to build the correspondences of two faces, the correspondences of two faces are referred as meshes instead of line pairs. Karungaru et al. \cite{mhf} detect five feature points as control points, then segment face images into several triangles according to control points, next warp the face images according to corresponding triangles. These face morphing methods have a different way in building correspondences, Their aim is creating a seamless transition from one face to the other.

\textbf{Face Swapping.} Face swapping also known as face replacement, means transferring a face from a source photo onto a face appearing in a target photo in order to generate a new genuine face.

The most early work for face swapping is \cite{exchface}, but the results are not that realistic. Later, the results are better \cite{de1}\cite{de2}\cite{de3}\cite{de4} and mainly used for deidentification. The main problem for face swapping is pose variation, most works solve this problem by using 3D models \cite{de3}\cite{3d1}\cite{onseg}. Nirkin et al. \cite{onseg} transfer the face from the source image onto the target image using a 3D face model, they first detect facial landmarks used to establish 3D pose and facial expression for 3D face shape, then use a FCN to segment the visible parts of faces from their context and occlusions, at last, source face is blended-in with the target context using image seamless cloning \cite{pie}, this step would fail when blending very different facial hues. Korshunova et al. \cite{faceswapping} propose a network to generate a source face that could replace the face in target image, one network could only generate faces of the same type, for example, a net named CageNet could only generate the face of Nicolas Cage. Those two methods mentioned above may greatly change the lightning condition of the face on the target image because they just transfer the source face onto the target image, but not containing any information of the target face. Bitouk et al. present a system for automatic  face replacement in images, given a target face image, the system would retrieve a face with similar pose, lightning and color condition, then use such a face to replace the face on the target image, the limitation of the source face is clear, it must have a similar pose and lightning condition with the target face. The main difference between face swapping and our task is that we want keep some features of the target face but not to replace the target face with the source face, how to fuse the features of the source faces into the target faces is our main consideration.

\section{Fusing features of the source face into the target face }
Our task is to fuse features of source face into target face, there are two steps to follow.

First, aligning two faces, it is necessary to calculate the location and the size of the facial part from each face image. With the help of the facial landmarks \cite{fld}, it is quick to do so. But only knowing the location and size of two faces is not enough, we need to know where are the eyes, nose and other parts of the face, different faces have different face features, some people have a big nose while others with a small one, some people's nose is closer to eyes while others' nose is farther to eyes. Still, with the help of facial landmarks, we know exactly the distribution of the face features.

Second, by aligning two faces, we know the features on one face and its corresponding features on the other face. Then, we could fuse one face features into the other face from feature to feature.

\begin{center}
    \includegraphics[width=4.8in]{images/pipeline.png}
    \fcaption{Pipeline of our work: (a) perform facial landmark detection on two faces. (b) triangulate two faces according to the facial landmarks. (c) calculate new facial landmarks by interpolating two groups of facial landmarks, warp two faces according to the new facial landmarks. (d) extract the facial part from the face image and the calculate corresponding facial part of the other image. (e) fuse one's face features into the other face by seamless cloning.}
\end{center}

\subsection{Face alignment}
There are many ways to find facial landmarks \cite{fld2}\cite{fld3}\cite{fld4}\cite{fld}, we adopt \cite{fld}, it is implemented in Dlib.

In Fig. 1(a), with the landmarks of two faces, two faces' bounding box could be calculated, as we talked before, different faces have different facial landmarks, so the source face's bounding box and the target face's bounding box could have different ratio of height and width. In order to align two faces easily, we reshape the bounding boxes of two faces into square by stretching the shorter side of the bounding box, next, we will call our bounding box of face as bounding square for its shape is square. Then, we need to enlarge the bounding squares of two face a little for the consensus of triangulation of two faces as shown in Fig. 1(b). The center point and side length of the bounding square of the source face are denoted as $c_s$ and $l_s$, the center point and side length of the bounding square of the target face are denoted as $c_t$ and $l_t$. The index of facial landmarks $i \in \{1...N\}$, $N$ is the number of facial landmarks, for each point $p_i$ on the source face bounding square area, we could calculate its corresponding location on the target face by following mapping function:
$$f(p_i)=(p_i-c_s) \cdot \frac{l_t}{l_s} + c_t \eqno{(1)}$$
Before we triangulate a face according to facial landmarks, we need make sure that the transition from the background to the face is natural, so we add some points into the facial landmarks to help triangulate the faces, these points are four corners of the face bounding square and four midpoints on bounding square side. The facial landmarks we need to triangulate the source face and the target face are denoted as $F_{s}$(the red points on the lower face image in Fig. 1(b)) and $F_{t}$(the red points on the upper face image in Fig. 1(b)) respectively. The facial landmarks we want could be calculated by interpolating $F_s$ and $F_t$ and are denoted as $F_n$, with these new facial landmarks, we could create a new face structure. for each point in $F_s$, $F_t$ and $F_n$ are denoted as $F_{s_i}$, $F_{t_i}$ and $F_{n_i}$ respectively.
$$F_{n_i} = \alpha \cdot f(F_{s_i})+(1-\alpha)\cdot F_{t_i} \eqno{(2)}$$
$\alpha$ controls which face structure we prefer more, if $\alpha$ is small, the new face structure would look more like the target face, in other words, the profile of the new face and the distribution of the new face features would look more like the target face.

After triangulating two faces according to $F_s$ and $F_t$ using Delaunay Triangulation, we get two groups triangles of the source face and the target face, denoted as $T_s$ and $T_t$ respectively. The two faces corresponding triangles are not of the same shape because of different facial landmarks. what we need to do is to make sure the shape of corresponding triangles of two faces are the same. we could get $T_n$ by triangulating $F_n$. Warping $T_{t}$ to the $T_{n}$, and warping $T_s$ in a similar way. In Fig. 1(c), the source face and the target face are warped, the size and the location of their bounding squares are different at each face image, but the shape of the corresponding triangles of two faces  are the same.

\subsection{Face fusion}
After warping two faces, we have two faces of the same structures and the same distribution of face features, but the size and the location of their bounding square are different, in convenience, we extract the bounding square area of the source face, scale and translate this area to the same size and location of the bounding square of the target face, the new source face image is shown in Fig. 2(b).

As we can see from the Fig. 2, the triangles of the source face and the target face are exactly at the same location and of the same shape and size. the purpose of our task is to fuse the source face features into the target face, so we need to know the profile of the source face, then extract the facial part from the source face image. In Fig. 2(c), the yellow outline is generated from the facial landmarks, but we need to shrink the profile a little to make sure that all the pixels are from the face, inside the blue outline is $\Omega$, $\Omega$ stands for the facial part we are going to fuse.

\begin{center}
    \includegraphics[width=4.8in]{images/extract.png}
    \fcaption{(a) the target face warped according to $F_n$. (b) the warped source face after being translated and scaled. (c) yellow outline is generated from the landmarks of the warped source face, light blue outline is shrank a little from the yellow profile. (d) facial part of the warped source face that inside $\Omega$. (e) facial mask.}
\end{center}

The target face and the source face are shown in Fig. 2(a) and 2(b), then we fuse the gradients of of two faces, recover the face from the fused gradients by seamless cloning.

The gradient of the source face is denoted as $g_s$, the gradient of the target face is denoted as $g_t$, $g$ is the gradient we will use to recover the face, there are three ways we tried to fuse the gradients of two faces.

For the first way, The guidance field $g$ could be generated  by linearly combinating $g_s$ and $g_t$:

$$g = \beta \cdot g_s+(1-\beta) \cdot g_t \eqno{(3)}$$

For the second way, at each point in $\Omega$, retain the stronger gradients in $g_s$ or in $g_t$:

$$
for\ all\ x \in \Omega\text{, } g(x)=\left\{
\begin{aligned}
g_s(x) &&{if |g_s(x)|>|g_t(x)|}\\
g_t(x) &&{else}\\
\end{aligned}
\right.
\eqno{(4)}$$

For the third way, define $g$ by linearly combination of $g_d$ and $g_t$:

$$g = \beta \cdot g_d+(1-\beta) \cdot g_t \eqno{(5)}$$

The definition of $g_d$ is:

$$ for\ all\ x \in \Omega\text{, } g_d(x)=\left\{
\begin{aligned}
g_t(x) &&{if \ |g_s(x)|< \theta_1 \ or \ |g_t(x)|< \theta_2}\\
g_s(x) &&{else}\\
\end{aligned}
\right.
\eqno{(6)}$$

$g_d$ contains the bigger gradients of the $g_s$ and lower gradients of $g_t$, if $g_s(x)$ is smaller than the threshold $\theta_1$ and $g_t(x)$ is smaller than the threshold $\theta_2$, then $g_d(x)$ keeps the gradient from $g_t$ at point $x$, so it contains main features of the source face and the part of the lightning condition of the target face. Fig. 3(a) and Fig. 3(b) are facial parts of the target face and source face, they share the same profile and distribution of features, Fig. 3(c) shows that which parts of $g_d$ are come from $g_t$ or $g_s$, for all $x \in \Omega$ and for each channel, if $g_d(x)$ is come from $g_s(x)$, then the corresponding part in Fig. 3(c) is brighter. \\
Finally, we could use $g$ to recover the face we want on the target face image by seamless cloning, we could control how much features from the source face to fuse into target face by using the first way or the third way.

\begin{center}
    \includegraphics[width=4.8in]{images/labelmask.png}
    \fcaption{(a) Facial part of target face. (b) Facial part of the source face. (c) A mask indicate which part of $g_d$ are come from $g_s$ or $g_t$.}
\end{center}

\section{Experiments}

\subsection{Comparison of different gradient reconstruction}
\begin{center}
    \includegraphics[width=4.8in]{images/3cmp.png}
    \fcaption{The effects of different gradient reconstruction: (a) Source face image. (b) Target face image. (c) Fusing face by linearly combining the gradients of two faces. (d) Fusing face by keeping the larger gradients of two faces. (e) Fusing face through weaker gradient preserving method.}
\end{center}

Fig. 4 shows the effects of different face gradient reconstruction. From Fig. 4(c), we can see that direct linear combination of two faces gradients could bring too much illumination from source face into target face, which makes face looks very unnatural. If we kept the larger gradients of two faces, the illumination of the target face would be strongly broken, more importantly, we can't control how much features we are going to fuse from source face into target face. In Fig. 4(e), the lightning condition and facial hue is closest to the target face image, when we reconstruct gradients using the third method, we only keep the larger gradients of the source face which means the main features of the source are preserved, and we keep lots of the weaker gradients of the target face, which means we maintain some illumination information and facial hue of the target face.

\subsection{How the different parameters effect face fusion}
In this paper, there are four parameters we can control, they are $\alpha$, $\beta$, $\theta_1$, $\theta_2$. $\alpha$ controls the structure of human faces, in other words, it controls the profile and the distribution of features of human faces. $\beta$ controls the details of human faces, like wrinkles and textures. With $\alpha$ and $\beta$, we can control how much content we want to fuse from source face into target face. If the gradients of source face is weaker than $\theta_1$ and the gradients of target face is weaker than $\theta_2$, then we retain the gradients of the target face, otherwise we keep the gradients of the source face.\\
Fig. 5 shows how $\alpha$ effects the fused results, $\beta = 0.5$, $\theta_1 = 10$, $\theta_2 = 15$. As the $\alpha$ grows bigger, the distribution of face features of fused face is more like the source face.

\begin{center}
    \includegraphics[width=4.8in]{images/pro.png}
    \fcaption{(a) Source face. (b) Target face. (c) Fused face, $\alpha = 0.2$. (d) Fused face, $\alpha = 0.4$. (e) Fused face, $\alpha = 0.6$. (f) Fused face, $\alpha = 0.8$.}
\end{center}

Fig. 6 shows how $\beta$ effects the fused result, $\alpha = 0.5$, $\theta_1 = 10$, $\theta_2 = 15$. As the $\beta$ grows bigger, the fused result has more details from source face.

\begin{center}
    \includegraphics[width=4.8in]{images/detail.png}
    \fcaption{(a) Source face. (b) Target face. (c) Fused face, $\beta = 0.2$. (d) Fused face, $\beta = 0.4$. (e) Fused face, $\beta = 0.6$. (f) Fused face, $\beta = 0.8$.}
\end{center}

If we want to only keep main features of source face, we could only retain stronger gradients of the source face, Fig. 7 shows that the fused results are more natural as more weaker gradients of the source face are been dropped. $\alpha = 0.5$, $\beta = 0.5$, $\theta_2 = 40$. The weaker gradients carry too much illumination information and facial hues of the source face, when these gradients are brought into the fused results, the results are become very unnatural.

\begin{center}
    \includegraphics[width=4.8in]{images/thd1.png}
    \fcaption{(a) Source face. (b) Target face. (c) Fused face, $\theta_1 = 2$. (d) Fused face, $\theta_1 = 4$. (e) Fused face, $\theta_1 = 6$. (f) Fused face, $\theta_1 = 8$.}
\end{center}

When we fuse the main features of source face into target face, somehow, we need to erase the main features of target face. In fact, we want to keep the illumination information of the target face, both the weaker gradients of source face and weaker gradients of target face are being kept. For Fig. 8, it shows the experiment on $\theta_2$,  $\alpha = 0.5$, $\beta = 0.5$, $\theta_1 = 10$, when the $\theta_2$ grows bigger, more stronger gradients are preserved, as we could see, more shadows are preserved.

\begin{center}
    \includegraphics[width=4.8in]{images/thd2.png}
    \fcaption{(a) Source face. (b) Target face. (c) Fused face, $\theta_2 = 10$. (d) Fused face, $\theta_2 = 20$. (e) Fused face, $\theta_2 = 30$. (f) Fused face, $\theta_2 = 40$.}
\end{center}

Although our work are not aiming at face swapping task, we still could create similar effects by setting $\beta = 1$, this means we almost preserving all the gradients of source faces, but only keep part of weaker gradients of target faces. We compare our results and the work of Korshunova et al. \cite{faceswapping}, in Fig. 9, the source face of all the results is from Fig. 8(a), we can see that our results keep more illumination information and facial hues of target faces which makes our results more natural, but in Fig. 9(c), our result fail to catch the lightning condition of the target face from a high level, this is where we need improve.

\begin{center}
    \includegraphics[width=4.8in]{images/vs.png}
    \fcaption{The first row are the original target face, the second row are the results of Korshunova et al., the third row are our results.}
\end{center}

\section{Conclusions}

We developed a face fusion method that could automatically fusing one face features into the other face. The 68 facial landmarks are enough to align two faces precisely. The illumination of the source face might pollute the target face, but the method we proposed preserves as much illumination information as possible while decreasing the affection of illumination of the source face. At mean time, we developed a user interface for letting users control how much features they want from the source face to fuse into the target face. However, we still can't handle the face occlusion problem, if one face was occluded by fingers or hairs, then the fusion results are not good. In face, if we can remove the illumination of two faces, fuse the source face features into the target face without illumination, then recover the fused face illumination by using the lighting condition of the target face, we believed the fused face would look more natural in this way.

\begin{thebibliography}{4}

\bibitem{fbim} Beier, T., Neely, S.: Feature-based image metamorphosis. In: ACM SIGGRAPH Computer Graphics, Vol. 26, No. 2, pp. 35-42 (1992)
\bibitem{wol}Wolberg, G.: Image morphing: a survey. In: The visual computer, 14(8), 360-372 (1998)
\bibitem{pie}P¨¦rez, P., Gangnet, M., Blake, A.: Poisson image editing. In: ACM Transactions on Graphics (TOG), Vol. 22, No. 3, pp. 313-318 (2003)
\bibitem{mhf}Karungaru, S., Fukumi, M., Akamatsu, N.: Morphing Human Faces: Automatic Control Points Selection And Color Transition. In: International Conference on Computational Intelligence, pp. 224-227 (2004)
\bibitem{exchface}Blanz, V., Scherbaum, K., Vetter, T., Seidel, H. P.: Exchanging faces in images. In: Computer Graphics Forum, Vol. 23, No. 3, pp. 669-676 (2004)
\bibitem{autorep}Bitouk, D., Kumar, N., Dhillon, S., Belhumeur, P., Nayar, S. K.: Face swapping: automatically replacing faces in photographs. ACM Transactions on Graphics (TOG), 27(3), 39. (2008)
\bibitem{de1}De La Hunty, M., Asthana, A., Goecke, R.: Linear facial expression transfer with active appearance models. In: International Conference on Pattern Recognition (ICPR) (2010)
\bibitem{de2}Ross, A., Othman, A.: Visual cryptography for biometric privacy. In: IEEE transactions on information forensics and security.(2011)
\bibitem{de3}Lin, Y., Wang, S., Lin, Q., Tang, F.: Face swapping under large pose variations: A 3D model based approach. In: International Conference on Multimedia and Expo (ICME), pp. 333-338 (2012)
\bibitem{3d1}Lin, Y., Lin, Q., Tang, F., Wang, S.: Face replacement with large-pose differences. In: Proceedings of the 20th ACM international conference on Multimedia. (2012)
\bibitem{fld2}Xiong, X., De la Torre, F.: Supervised descent method and its applications to face alignment. In: Proceedings of the IEEE conference on computer vision and pattern recognition.(2013)
\bibitem{fld3}Ren, S., Cao, X., Wei, Y., Sun, J.: Face alignment at 3000 fps via regressing local binary features. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.(2014).
\bibitem{fld4}Cao, X., Wei, Y., Wen, F., Sun, J.: Face alignment by explicit shape regression. In: International Journal of Computer Vision. (2014)
\bibitem{de4}Mosaddegh, S., Simon, L., Jurie, F.: Photorealistic Face de-Identification by Aggregating Donors¡¯ Face Components. In: Asian Conference on Computer Vision, pp. 159-174 (2014)

\bibitem{fld}Kazemi, V., Sullivan, J.: One millisecond face alignment with an ensemble of regression trees. In: Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR), pp. 1867-1874 (2014)

\bibitem{faceswapping}Korshunova, I., Shi, W., Dambre, J., Theis, L.: Fast face-swap using convolutional neural networks. arXiv preprint arXiv:1611.09577. (2016)
\bibitem{onseg}Nirkin, Y., Masi, I., Tran, A. T., Hassner, T., Medioni, G.: On Face Segmentation, Face Swapping, and Face Perception. arXiv preprint arXiv:1704.06729. (2017)


\end{thebibliography}



\end{document}
